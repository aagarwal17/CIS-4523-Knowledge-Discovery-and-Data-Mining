{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6cc4b7-ce57-4a07-9881-89fa61a1a9c8",
   "metadata": {},
   "source": [
    "# Models for Student Performance on Examinations Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdeae67-168d-4e54-bf73-dd96e0bc6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "\n",
    "#Basic Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Preprocessing Imports\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "# Metrics Import\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ML Model Imports\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor \n",
    "import xgboost as xg \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pylab import rcParams\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c9501-7b0a-4213-8c52-542dc87beeb9",
   "metadata": {},
   "source": [
    "## Reading in Data/CSV File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95934072-bb7a-4cae-a9fb-941e49d40b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>average score</th>\n",
       "      <th>Math_Pass_Status</th>\n",
       "      <th>Reading_Pass_Status</th>\n",
       "      <th>Writing_Pass_Status</th>\n",
       "      <th>Overall_Pass_Status</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>72.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>82.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>92.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>49.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>76.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>57.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>71</td>\n",
       "      <td>65</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>78</td>\n",
       "      <td>77</td>\n",
       "      <td>74.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender  race/ethnicity  parental level of education  lunch  \\\n",
       "0         0               1                            1      1   \n",
       "1         0               2                            4      1   \n",
       "2         0               1                            3      1   \n",
       "3         1               0                            0      0   \n",
       "4         1               2                            4      1   \n",
       "..      ...             ...                          ...    ...   \n",
       "995       0               4                            3      1   \n",
       "996       1               2                            2      0   \n",
       "997       0               2                            2      0   \n",
       "998       0               3                            4      1   \n",
       "999       0               3                            4      0   \n",
       "\n",
       "     test preparation course  math score  reading score  writing score  \\\n",
       "0                          1          72             72             74   \n",
       "1                          0          69             90             88   \n",
       "2                          1          90             95             93   \n",
       "3                          1          47             57             44   \n",
       "4                          1          76             78             75   \n",
       "..                       ...         ...            ...            ...   \n",
       "995                        0          88             99             95   \n",
       "996                        1          62             55             55   \n",
       "997                        0          59             71             65   \n",
       "998                        0          68             78             77   \n",
       "999                        1          77             86             86   \n",
       "\n",
       "     average score  Math_Pass_Status  Reading_Pass_Status  \\\n",
       "0        72.666667                 1                    1   \n",
       "1        82.333333                 1                    1   \n",
       "2        92.666667                 1                    1   \n",
       "3        49.333333                 0                    0   \n",
       "4        76.333333                 1                    1   \n",
       "..             ...               ...                  ...   \n",
       "995      94.000000                 1                    1   \n",
       "996      57.333333                 1                    0   \n",
       "997      65.000000                 0                    1   \n",
       "998      74.333333                 1                    1   \n",
       "999      83.000000                 1                    1   \n",
       "\n",
       "     Writing_Pass_Status  Overall_Pass_Status  Grade  \n",
       "0                      1                    1      2  \n",
       "1                      1                    1      1  \n",
       "2                      1                    1      0  \n",
       "3                      0                    0      5  \n",
       "4                      1                    1      2  \n",
       "..                   ...                  ...    ...  \n",
       "995                    1                    1      0  \n",
       "996                    0                    0      4  \n",
       "997                    1                    0      3  \n",
       "998                    1                    1      2  \n",
       "999                    1                    1      1  \n",
       "\n",
       "[1000 rows x 14 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/dataEncoded.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c5e69-9496-499d-842e-795bd82a8a35",
   "metadata": {},
   "source": [
    "## Linear Regression: <a id = 'linearreg'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cdcf50-677d-4adc-850b-57c21231a9de",
   "metadata": {},
   "source": [
    "I am creating a linear regression model to determine if one can use categorical features--gender, race/ethnicity, parental level of education, having or not having lunch, and taking or not taking a test preparation course--to predict student performance on an examination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bde901-33ef-44d1-8a95-5e4342c2516d",
   "metadata": {},
   "source": [
    "### Splitting the Dataset: <a id = 'linsplit'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a1eadb1-1b13-4872-81df-5bc9b1bbc4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5)\n",
      "(200, 5)\n"
     ]
    }
   ],
   "source": [
    "#The dataset will be split into a training set with 80% of samples and a test set with 20% of samples\n",
    "#The training set is used to learn model parameters and the testing set is used to evaluate the learned model\n",
    "\n",
    "#Splitting the samples:\n",
    "student_performance_fea = df.drop(['reading score','writing score','math score','average score', 'Math_Pass_Status', 'Reading_Pass_Status', 'Writing_Pass_Status', 'Overall_Pass_Status', 'Grade'], axis=1).values\n",
    "target = df['average score'].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(student_performance_fea,\n",
    "                                                 target,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "#It makes sense that there are 800 values in the training set because this is 80% of the total number of values.\n",
    "#It then also makes sense that there are 200 values in the testing set because this is 20% of the total number of values.\n",
    "\n",
    "\n",
    "#Normalizing the features:\n",
    "normalizer = StandardScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401787dd-fddc-4634-9353-42bee4038a64",
   "metadata": {},
   "source": [
    "### Training the linear regression model: <a id = 'lintrain'></a>\n",
    "\n",
    "I will use the Linear regression model to do prediction\n",
    "\n",
    "$\\min_{w}\\frac{1}{n}\\|y-X\\mathbf{w}\\|_2^2$\n",
    "\n",
    "I will output the learned model parameter $\\mathbf{w}$ and see how the learned model fit the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1126aa76-d6d5-4f23-84d3-217168e0f204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learned model parameter w: \n",
      "Bias is 68.16916666666665\n",
      "Coefficients are [-2.08400337  2.21246318 -0.95409731  4.32734641 -3.90283275]\n",
      "Prediction for the training set:\n",
      "MAE is: 10.050407732008344\n",
      "MSE is: 154.94186635100735\n",
      "RMSE is: 12.447564675510119\n"
     ]
    }
   ],
   "source": [
    "#Training the model:\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train,Y_train)\n",
    "\n",
    "print(\"For learned model parameter w: \")\n",
    "print(\"Bias is \" + str(lr.intercept_))\n",
    "print(\"Coefficients are \" + str(lr.coef_))\n",
    "\n",
    "Y_train_pred = lr.predict(X_train)\n",
    "\n",
    "mae = mean_absolute_error(Y_train_pred, Y_train)\n",
    "mse = mean_squared_error(Y_train_pred, Y_train)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('Prediction for the training set:')\n",
    "print('MAE is: {}'.format(mae))\n",
    "print('MSE is: {}'.format(mse))\n",
    "print('RMSE is: {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26cd6d-f94c-4518-87ca-88715ca18541",
   "metadata": {},
   "source": [
    "While not much information can be collected from these scores alone, they wil be compared to results from other models such as Ridge Regression, SVM, and Random Forest Regression to determine their validity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93316c6d-6c11-4191-956d-45a08d951c40",
   "metadata": {},
   "source": [
    "### Evaluating the linear regression model: <a id = 'linevaluate'></a>\n",
    "\n",
    "\n",
    "I will evaluate the learned model to see how well this model generalizes on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7aece421-42fb-4061-b2f0-34828a5b00f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the testing set:\n",
      "MAE is: 10.717163335369497\n",
      "MSE is: 187.4495832223678\n",
      "RMSE is: 13.691222853433064\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Evaluating the model to see how well it generalizes on the testing set:\n",
    "\n",
    "Y_test_pred = lr.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(Y_test, Y_test_pred)\n",
    "mse = mean_squared_error(Y_test, Y_test_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('Prediction for the testing set:')\n",
    "print('MAE is: {}'.format(mae))\n",
    "print('MSE is: {}'.format(mse))\n",
    "print('RMSE is: {}'.format(rmse))\n",
    "\n",
    "labels = ['Person1', 'Person2', 'Person3', 'Person4', 'Person5', 'Person6', 'Person7', 'Person8', 'Person9', 'Person10']\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.4  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "rects1 = ax.bar(x - width/2, Y_test[0:10], width, label='Ground Truth')\n",
    "rects2 = ax.bar(x + width/2, Y_test_pred[0:10], width, label='Prediction')\n",
    "\n",
    "ax.set_title(\"Linear Regression\")\n",
    "ax.set_xlabel(\"Data Sample\")\n",
    "ax.set_ylabel('Average/Mean Score')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "#From this plot, we can see the learned model does well to generalize on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b87a0d-d845-489e-8220-3e2c64355f25",
   "metadata": {},
   "source": [
    "By comparing the ground truth and prediction for a sample of the data, one can see that the learned linear regression model does well to generalize on the testing set. In other words, it is satisfactory to use the categorical features in the dataset to predict student performance on examinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a521da-8a26-4261-b44b-87a66e1ede47",
   "metadata": {},
   "source": [
    "## Ridge Regression: <a id = 'ridge'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d765e8-fa6b-4999-9854-ebc6aa194b3d",
   "metadata": {},
   "source": [
    "### Training the Ridge Regression Model: <a id = 'ridgetrain'></a>\n",
    "$\\min_{w}\\frac{1}{n}\\|y-Xw\\|_2^2 + \\lambda \\|w\\|_2^2$\n",
    "\n",
    "I will compare its performance on the testing set with that of the standard linear regression model $\\min_{w}\\frac{1}{n}\\|y-Xw\\|_2^2$\n",
    "\n",
    "I will also use different $\\lambda$ to see how it affects the performance of the ridge regression  model on the testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b6470bc-4483-4df6-ba9d-bcde1a277487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learned model parameter w: \n",
      "Bias is 68.16916666666665\n",
      "Coefficients are [-2.08373712  2.21222729 -0.95399722  4.32679848 -3.90233655]\n",
      "Prediction for the training set:\n",
      "MAE is: 10.050373148176108\n",
      "MSE is: 154.94186703591038\n",
      "RMSE is: 12.447564703021646\n"
     ]
    }
   ],
   "source": [
    "#Training the model with Ridge Regression:\n",
    "\n",
    "rr = Ridge(alpha = 0.1)\n",
    "\n",
    "rr.fit(X_train,Y_train)\n",
    "\n",
    "print(\"For learned model parameter w: \")\n",
    "print(\"Bias is \" + str(rr.intercept_))\n",
    "print(\"Coefficients are \" + str(rr.coef_))\n",
    "\n",
    "Y_train_pred2 = rr.predict(X_train)\n",
    "\n",
    "mae2 = mean_absolute_error(Y_train_pred2, Y_train)\n",
    "mse2 = mean_squared_error(Y_train_pred2, Y_train)\n",
    "rmse2 = np.sqrt(mse2)\n",
    "\n",
    "\n",
    "print('Prediction for the training set:')\n",
    "print('MAE is: {}'.format(mae2))\n",
    "print('MSE is: {}'.format(mse2))\n",
    "print('RMSE is: {}'.format(rmse2))\n",
    "\n",
    "#Using Ridge Regression the prediction values for the training set for MAE, MSE, and RMSE are almost idential for the values for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649aa6ad-1ca2-4c4a-8455-17299ed76fc7",
   "metadata": {},
   "source": [
    "Similar to the linear regression model, not much information can be gathered from these scores/values alone. Thus, they will be compared to the other models I create to determine their validity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f507c626-bbd7-4210-ad7a-f47d3616f014",
   "metadata": {},
   "source": [
    "### Evaluating the Ridge Regression Model: <a id = 'rideval'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95cde286-9d31-4f88-91fd-d8b5c0b41e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the testing set:\n",
      "MAE is: 10.717076759522906\n",
      "MSE is: 187.44770619143767\n",
      "RMSE is: 13.691154304566057\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Difference between Ridge Regression and Linear Regression: \n",
      "Percent Difference for MAE is: 0.0008078275339647847%\n",
      "Percent Difference for MSE is: 0.0010013574294264152%\n",
      "Percent Difference for RMSE is: 0.0005006787147225711%\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model to see how well it generalizes on the testing set:\n",
    "\n",
    "Y_test_pred2 = rr.predict(X_test)\n",
    "\n",
    "mae2 = mean_absolute_error(Y_test, Y_test_pred2)\n",
    "mse2 = mean_squared_error(Y_test, Y_test_pred2)\n",
    "rmse2 = np.sqrt(mse2)\n",
    "\n",
    "print('Prediction for the testing set:')\n",
    "print('MAE is: {}'.format(mae2))\n",
    "print('MSE is: {}'.format(mse2))\n",
    "print('RMSE is: {}'.format(rmse2))\n",
    "\n",
    "labels = ['Person1', 'Person2', 'Person3', 'Person4', 'Person5', 'Person6', 'Person7', 'Person8', 'Person9', 'Person10']\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.4  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "rects1 = ax.bar(x - width/2, Y_test[0:10], width, label='Ground Truth')\n",
    "rects2 = ax.bar(x + width/2, Y_test_pred2[0:10], width, label='Prediction')\n",
    "\n",
    "ax.set_title(\"Ridge Regression\")\n",
    "ax.set_xlabel(\"Data Sample\")\n",
    "ax.set_ylabel('Average/Mean Score')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "#From this plot, we can see the learned model does well to generalize on the testing set.\n",
    "\n",
    "#The Ridge Regression and Linear Regression produce almost identifical adults with alpha =0.1, so both perform well. \n",
    "#To compare the ridge regression and linear regression models, I will use percent difference calculations:\n",
    "#Percent Difference = (|Value 1 – Value 2|)/([Value 1 + Value 2]/2) * 100%\n",
    "\n",
    "print(\"Percent Difference between Ridge Regression and Linear Regression: \")\n",
    "maepd = ( (abs(mae - mae2))/((mae + mae2)/2) ) * 100\n",
    "msepd = ( (abs(mse - mse2))/((mse + mse2)/2) ) * 100\n",
    "rmsepd = ( (abs(rmse - rmse2))/((rmse + rmse2)/2) ) * 100\n",
    "print(\"Percent Difference for MAE is: {}%\".format(maepd))\n",
    "print(\"Percent Difference for MSE is: {}%\".format(msepd))\n",
    "print(\"Percent Difference for RMSE is: {}%\".format(rmsepd))\n",
    "#Since these percent difference values are so small, one can say that ridge regression and linear regression perform almost identically\n",
    "\n",
    "\n",
    "\n",
    "#I was unable to show the work of me changing the alpha values.\n",
    "#It would not print out the updates error values for the new alpha values.\n",
    "#However, by testing different lambdas, one notices that as lambda values increases, the performance of the ridge regression model on the testing set worsens.\n",
    "#This is because if one's lambda value is too high, the model will be simple, but they run the risk of underfitting the data. It will not learn enough about the training data to make useful predictions\n",
    "#Lambda values becoming incrementally smaller than 0.1 were also tested. \n",
    "#Once the lambda values became extremely small, the model became more complex and ran the risk of overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa25bf-2301-4b74-a06f-7ae151cbdbe8",
   "metadata": {},
   "source": [
    "By comparing the ground truth and prediction for a sample of the data, one can see that the learned ridge regression model does well to generalize on the testing set. In other words, it is satisfactory to use the categorical features in the dataset to predict student performance on examinations. Also, the small percent difference values between the ridge and linear regression models suggest they perform very similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badbeeaf-1644-433e-8e19-6583a3953895",
   "metadata": {},
   "source": [
    "## Logistic Regression #1: Student Exam Pass Status Based on Categorical Features: <a id = 'logisticcat'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92de112-ae46-4095-b73f-d5757bd508a4",
   "metadata": {},
   "source": [
    "Here, I will build a logisitic regression model to determine which categorical features do the best job at classifying students as passing or failing their examinations. I will use the extended dataframe I made so that I have access to Pass/Fail status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1536f-0b44-43e6-9468-b0b3d666f787",
   "metadata": {},
   "source": [
    "### Splitting the Dataset: <a id = 'logisticcatsplit'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "026615d9-dafd-461d-a952-05ffb0a8993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val: 800, test: 200\n",
      "(200, 5)\n",
      "(800, 5)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the samples:\n",
    "target_fea = df.drop(['Math_Pass_Status','Reading_Pass_Status', 'Writing_Pass_Status', 'Overall_Pass_Status', 'Grade', 'math score', 'reading score', 'writing score', 'average score'], axis=1).values\n",
    "target_prediction = df['Overall_Pass_Status'].values\n",
    "\n",
    "\n",
    "#Here, we use 20% of samples as the testing set and use the remaining samples to train the logistic regression model.\n",
    "#Thus, there are 200 of the samples in the testing set and 800 of the samples to train the logistic regression model. This is important for later so that we can evenly split into 10 folds.\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(target_fea, target_prediction, \n",
    "                                                            test_size=0.20, \n",
    "                                                            random_state=0)\n",
    "print(\"train_val: {}, test: {}\".format(X_train_val.shape[0], X_test.shape[0]))\n",
    "\n",
    "#Normalizing the features:\n",
    "normalizer = StandardScaler()\n",
    "X_train_val = normalizer.fit_transform(X_train_val)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_train_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b5a10-aea6-4b64-bb37-e3ffa433a5f4",
   "metadata": {},
   "source": [
    "### Training the logistic regression model and selecting the hyperparameter with cross-validation: <a id = 'logisticcattrain'></a>\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\min_{\\mathbf{w}} \\sum_{i=1}^{n}\\{\\log(1+\\exp(\\mathbf{w}^T\\mathbf{x}_i))-y_i\\mathbf{w}^T\\mathbf{x}_i \\} + \\lambda\\|\\mathbf{w}\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "* I will use the 10-fold cross-validation to select the hyperparameter $\\lambda$.\n",
    "* Furthermore, I will search $\\lambda$ from $\\{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 20, 50, 100\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e21eaa1-e3c2-42e0-ae69-22b7775a20f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[726 300 371  35 774  88 131   9 434 688 568 103 327 700 705 267 690 523\n",
      "  275 736 393 292 775  97 521 562 412  20 365 317 355 651 538 441 177 537\n",
      "  350 658 572 368 124 729 130 135 668 447 767 642 696 194  81 265 328 464\n",
      "  296 178 662 753  76 556 295 759 403 118 640  59 228 232   3 420 128 585\n",
      "  433 119 519 224 503 628 432 532]\n",
      " [366 165 469 737 437 492 256 416 394  38 744 741 666 276 347 606 253  27\n",
      "  734 287 203 577 545 633 151 677 121 414 716 336 320 153 581 468 423 435\n",
      "  302 425 190 142 500 712 515 133 299 156  26 242 159 505 192 458  55 404\n",
      "  496 671 318 161 613 748 289 689 743 173 667 209 379  11 288 781 681  51\n",
      "  409 171 401 670 462 605 152 390]\n",
      " [ 79 552 594 481 779 175 100 463 479 245 576 570 655  43 602 457 450 326\n",
      "  309 187 184 470 698 678 402 284 191 573 614 484  91 246 354 250 795 315\n",
      "  216 269  60 321 501 168 241 785 788 301 448 210 571 625 316 593 202 132\n",
      "  499 158 451 257 431 290 738 644  14 333  18 293  69 231 154 411 426  25\n",
      "  436 507  49 362 247 510 723 599]\n",
      " [233 195 473 186 283 490 566  41 620 141 378 475 750 421 382 218 308 487\n",
      "   98  36 598 452 553 782 337 765 709  40 150 516 259 580 777 160 535 148\n",
      "  582 474  66 491 188 215 134 595 672 331 304 346 127 419 652 511 656  31\n",
      "  117 176 205 646 686   4 731  73 720 319 120 626 478 225 489 520 422  46\n",
      "  579 639  77 565 418 207 102 220]\n",
      " [702 112  74  50 485 169 588  83   2 359 358 648 214  92 196 105  42 380\n",
      "  638  48 260 791 508 364 109 376 477 353 427 227 663 314 282 180 235  62\n",
      "  669  54 243 772 146 392 687 792 199  96 286 746 222 339 261 509 212 115\n",
      "  693 410 542 399 564 406 493 607 343 438  57 486 596 634 617 675 374 126\n",
      "  563  15 280 685 335 769   5 704]\n",
      " [206 754 106 530  19 123 108 428  68 252  78 182 264 223 334 740 546 291\n",
      "   30 526 560 561 408 592 200 325  34 266 664 657 143 285 279 635 226 524\n",
      "   47 590 144 114 239 255 155  99 694 608 732 541 453 697 735  23 101 665\n",
      "  711 502 692 310  22 217 183 213  86 763 601 776 727 755 528 116 543 373\n",
      "  164   7 615 297 361 706 637 307]\n",
      " [454 619 616 258 237 710 622 306 113 482 584 189 445 166  72 277 641 618\n",
      "  273 244  67 375 659 338 219 764 540  93 647 757 172 281 466 254 352 449\n",
      "  794 456 248 609 476 793 305 397  52 389  56 591 514 536 554 303  70 708\n",
      "  385 179 756 758 384 674 621 391 725 680 555 768 221 367 770 722 370 471\n",
      "  369 786 125 604 695 771 630 699]\n",
      " [229 405 597 733 263 612 589 504 691 784 629 136 636 717 349  85 324  87\n",
      "  236 745 798 198  17 204 517 201  82 624 766 742 270 329 208 249 518 676\n",
      "   95 796 583 230 631 558 107 512 752 140 569 527 627 332 751 162  28 439\n",
      "  311 461 460 357 679 465 139  63 442 395 603 170 122 529 780 547 762 377\n",
      "  398 760 149 313 550 713 531  58]\n",
      " [ 80 240 381 147  65 730 778  24 356 424 719 789  33 483 721 330  29 413\n",
      "  650 559 715 773 548   8 443 724 707 363 728  64 632 111   0 430 649 739\n",
      "  278 703 262 761 197 714 137  10  45 472 387 455 790 157 181 495 557 238\n",
      "  645 506 294 747  61 513 544 415 234 360  13 372 643  90 600 193  16 344\n",
      "  586 396 467  75 400 567 783 342]\n",
      " [174 268 673 498  84 185 660  32 522 383 429 684 533 497 610 682 653 551\n",
      "  701 444 525 661 407  12 322 578 539 654 623 211 494 534 351 312  37 480\n",
      "  271 488 587 440 298  21 129 145 683  94  44 104 459 749 323   6  71 386\n",
      "  611 797 446   1 167 251 718 341 574 345 138 163 799 348 340 575 549 388\n",
      "   39 110 272 417 787  89 274  53]]\n",
      "reg_coeff: 99999.99999999999, acc: 0.603\n",
      "reg_coeff: 1000.0, acc: 0.603\n",
      "reg_coeff: 100.0, acc: 0.666\n",
      "reg_coeff: 10.0, acc: 0.671\n",
      "reg_coeff: 1.0, acc: 0.671\n",
      "reg_coeff: 0.1, acc: 0.671\n",
      "reg_coeff: 0.05, acc: 0.671\n",
      "reg_coeff: 0.02, acc: 0.671\n",
      "reg_coeff: 0.01, acc: 0.671\n",
      "Best Accuracy: 0.6712 \n",
      "Best Reg: 0.1\n"
     ]
    }
   ],
   "source": [
    "#We need to learn the model parameter  𝐰 . \n",
    "#However, with different hyperparameters  𝜆 , we can get different model parameter  𝐰 , resulting in different prediction performance. \n",
    "#Thus, we will use the 10-fold cross-validation to select the hyperparameter  𝜆 .\n",
    "\n",
    "#Here we set the folds equal to 10 for 10-fold cross-validation\n",
    "folds = 10\n",
    "\n",
    "#We get the number of samples in the training and validation set\n",
    "num_train_val = X_train_val.shape[0] \n",
    "\n",
    "#Now, we shuffle the index of samples in the train_val set\n",
    "index_of_samples = np.arange(num_train_val) \n",
    "shuffle(index_of_samples)\n",
    "\n",
    "#We split the index of the train_valid set into 10 folds\n",
    "index_of_folds = index_of_samples.reshape(folds, -1)\n",
    "print(index_of_folds)\n",
    "\n",
    "#As suggested above, the hyperparameters chosen are listed below\n",
    "regularization_coefficient = [10**(-5), 10**(-3), 10**(-2), 10**(-1), 1, 10, 20, 50, 100]\n",
    "\n",
    "#Variables we create to store the values of the best accuracy and best regression:\n",
    "best_acc = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "for reg in regularization_coefficient:\n",
    "    #10-fold cross-validation\n",
    "    sum_acc = 0.0\n",
    "    for fold in range(folds):\n",
    "        \n",
    "        index_of_folds_temp = index_of_folds.copy()\n",
    "        \n",
    "        #We are getting the index of the validation set and storing it in a variable valid_index\n",
    "        valid_index = index_of_folds_temp[fold,:].reshape(-1) \n",
    "        #We are getting the index of the training set and storing it in a variable train_index\n",
    "        train_index = np.delete(index_of_folds_temp, fold, 0).reshape(-1)\n",
    "        \n",
    "        #Our training set:\n",
    "        X_train = X_train_val[train_index]\n",
    "        y_train = y_train_val[train_index]\n",
    "        \n",
    "        #Our validation set:\n",
    "        X_valid = X_train_val[valid_index]\n",
    "        y_valid = y_train_val[valid_index]\n",
    "                \n",
    "        #We write this to build the model with different hyperparameters:\n",
    "        clf = LogisticRegression(penalty='l2', C=reg, solver='lbfgs')\n",
    "        \n",
    "        #Train the model with the training set:\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_pred = clf.predict(X_valid)\n",
    "        acc = accuracy_score(y_valid, y_valid_pred)\n",
    "        \n",
    "        sum_acc += acc\n",
    "    \n",
    "    cur_acc = sum_acc / folds\n",
    "    \n",
    "    print(\"reg_coeff: {}, acc: {:.3f}\".format(1.0/reg, cur_acc))\n",
    "    \n",
    "    #We now want to store the best hyperparameter:\n",
    "    if cur_acc > best_acc:\n",
    "        best_acc = cur_acc\n",
    "        best_reg = reg\n",
    "        \n",
    "print(\"Best Accuracy: {:.4f} \".format(best_acc))\n",
    "print(\"Best Reg: {:}\".format(best_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3df648-3fe9-4839-91d4-dc96773bcc82",
   "metadata": {},
   "source": [
    "### Retraining and Evaluating the Model with the Best Hyperparamter: <a id = 'logisticcatretrain'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c20b76b-dd4b-4c05-8c5c-8b12d98a83a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.650, recall: 0.802, precision: 0.678, f1: 0.735,\n",
      "Our learned model parameter vector 'w':  [0.13543526 0.2805534  0.08924401 0.51588381 0.40817753]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now that we have the best hyperparameter, we retrain the model:\n",
    "clf = LogisticRegression(penalty='l2', C=best_reg, solver='lbfgs')\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "#Visualizing the elements of the learned model parameter vector w with a bar plot:\n",
    "print(\"accuracy: {:.3f}, recall: {:.3f}, precision: {:.3f}, f1: {:.3f},\".format(acc, recall, precision, f1))\n",
    "\n",
    "print(\"Our learned model parameter vector 'w': \", abs(clf.coef_[0]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "labels = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']\n",
    "coefficients = abs(clf.coef_[0])\n",
    "ax.bar(labels, coefficients)\n",
    "plt.title(\"Bar Graph of Elements of the Learned Model Parameter Vector 'w'\")\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a0f64-8e9e-4556-baec-fbcd72c2d75b",
   "metadata": {},
   "source": [
    "From this plot, we can see that some features have much greater coefficients in the learned model parameter than others.\n",
    "Specifically, lunch and test preparation course have the higher coefficient values.\n",
    "This means that these features were able to fit the logistic regression model with the hyperparameter best of all of the features. One could say having or not having lunch and/or a test preparation course are then good predictors of whether a student passes or fails an exam.\n",
    "It also tells us that lunch and test preparation course are more statistically significant than the other features. The closer the coefficient values are to 0, the less correlation exists (that is why we took the absolute value).\n",
    "\n",
    "It should be noted that, although we took the absolute value of the coefficients, a positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. \n",
    "A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76679682-0293-4578-bc9f-c9bfde2e0471",
   "metadata": {},
   "source": [
    "## Logistic Regression #2: Gender Prediction Based on Exam Scores: <a id = 'logisticgender'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d24c9-c1e0-48b0-9a3f-0de3bf3ea170",
   "metadata": {},
   "source": [
    "Here, I will build another logisitic regression model; however, this time I will try to determine if one's gender can be classified/predicted based on their examination scores. I will use the extended dataframe I made so that I have access to Pass/Fail status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d8906-7a9f-4f88-90b1-1e4cf989e6a4",
   "metadata": {},
   "source": [
    "### Splitting the Dataset: <a id = 'logisticgendersplit'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6016d65-34fd-4a91-aa42-3cbc596ea9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val: 800, test: 200\n",
      "(200, 4)\n",
      "(800, 4)\n"
     ]
    }
   ],
   "source": [
    "#Can we predict one's gender based on their math, reading, and writing scores?\n",
    "#Splitting the samples:\n",
    "target_fea = df.drop(['Math_Pass_Status','Reading_Pass_Status', 'Writing_Pass_Status', 'Overall_Pass_Status', 'Grade', 'gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'], axis=1).values\n",
    "target_prediction = df['gender'].values\n",
    "\n",
    "\n",
    "#Here, we use 20% of samples as the testing set and use the remaining samples to train the logistic regression model.\n",
    "#Thus, there are 200 of the samples in the testing set and 800 of the samples to train the logistic regression model. This is important for later so that we can evenly split into 10 folds.\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(target_fea, target_prediction, \n",
    "                                                            test_size=0.20, \n",
    "                                                            random_state=0)\n",
    "print(\"train_val: {}, test: {}\".format(X_train_val.shape[0], X_test.shape[0]))\n",
    "\n",
    "#Normalizing the features:\n",
    "normalizer = StandardScaler()\n",
    "X_train_val = normalizer.fit_transform(X_train_val)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_train_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fee387e-62fc-4422-9233-19b88a8b69de",
   "metadata": {},
   "source": [
    "### Training the logistic regression model and selecting the hyperparameter with cross-validation: <a id = 'logisticgendertrain'></a>\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\min_{\\mathbf{w}} \\sum_{i=1}^{n}\\{\\log(1+\\exp(\\mathbf{w}^T\\mathbf{x}_i))-y_i\\mathbf{w}^T\\mathbf{x}_i \\} + \\lambda\\|\\mathbf{w}\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "* I will use the 10-fold cross-validation to select the hyperparameter $\\lambda$.\n",
    "* Furthermore, I will search $\\lambda$ from $\\{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 20, 50, 100\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c49c8a3-456d-42ca-ad18-6697051a56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[285 339 270 378 748 250 156  32 208 216 492 553   6 665  80  56 255 217\n",
      "  563   0 770  45 729 140 453  22  53 502 312 406 586 750 565 608 295 133\n",
      "   30 294 774 526 281 139 617 491 732 152 315 206 233 199 637 789 110 680\n",
      "  546 797 397  34 528 416 390  76 322 520 150  15  43 562 287 795 301 513\n",
      "  671 400 609 471 379 182 372  74]\n",
      " [106 348 726 368  19 211 452 167 411 256 344 550 203 724 112 496 375 450\n",
      "    7 522 642 247 559 284  71 584 742 554 218 157 266  31 588 259 505 615\n",
      "  696 387 613 330 299 145 779 530 429   8 277   3 353 161 286 515  95 648\n",
      "  768 160 382 267 307 501 296 310  62 518 175 489 363 486 604 204 421 506\n",
      "   18  89 590 371 683 652  29  60]\n",
      " [305 148 262 719 763 758 682 610 485  55 753 463 431   5 695 778 761  61\n",
      "  523 282 508 413 661  24 380 788 428 626 745 215 766 747 739 468 635 632\n",
      "  601 228 548 524 542 708 516 558 706 395 583 735 456  57  83  44 646 657\n",
      "   46 594 576 460 769 691 137 422 587 325 291 710 433  86 715 127 573  13\n",
      "  479 578 495 404 622 462 290 180]\n",
      " [151 539 298 531 408 350 733 420 534  97 600 331 235 656 111 308 134 494\n",
      "  292 116 703 498 168 107 108 252 644 184  25 222 448 636 757 268 655 362\n",
      "  628 366 260 188 248 638 360 144 169 103 503 647 125 734 684 631  87 302\n",
      "  173 598  21 289 675 581 679 370  54 186 759 782  39  78 126 388 410 662\n",
      "  392  70 146 616 599 153  99 324]\n",
      " [ 17 792 109 346 264 101 321 591 490 343 749 541 254 320 337 694 507 741\n",
      "  261  37 668 728 113  98  67 464 347 551 796 484 265 469 418 440 725 709\n",
      "  122 381  20 317 650 504 545 142 213 572 434 470 640 672 224 423 773  69\n",
      "  645 641 574 399 214 288 589  28 564 444 205 141 467  88 606 271 639 634\n",
      "  407 633 396 251  93 603 790 436]\n",
      " [ 11 535 673 342 232 538 509 334   1 364 595 313 466 441 227 568 514 525\n",
      "  117 544 569 115 607 478 771   2 627 351 221 475 273 540 393 611 718 570\n",
      "  624  82 193 799 219 412 555 124 280 618 398 417 736 386 658 659 105 170\n",
      "  345 332 585 674 702  77 198 229 712 676 191 177 740 629   4 687  47 269\n",
      "  743 136 556 359 473 183 118  63]\n",
      " [488 190 727 245 253 744  59 154 435 756 374 383 415 225 720 171 135 722\n",
      "   12 785 791 389  49 596 278 223  36 258 580 623  94  72 552 533 419  92\n",
      "    9 698 667 663 241 272 304 257 593  79 549 537 236 414 566  16 181 309\n",
      "  567 121 510 592 760 582 202 149 794 451 685 716 197 147 163 442 443 738\n",
      "  499  23 723 605 130 461 174 314]\n",
      " [612 783 688 705 242  51 536 686 176 445 472 178  85 220 355  42 249 102\n",
      "   40 333 165 677 162 185  26 737 196 316  50 195 303 704 237 274 465 621\n",
      "  158 425 311 143 772 352 172 138 476 700 678 560 394 597 697 427 243 532\n",
      "  699 297 240 279 777 707 781 164  58 238 132 543 577 579 391 752 625 100\n",
      "  477 403  81 300 446 276 643 457]\n",
      " [649 651 493 776 201 166 319 226 731 762 246 338 212 798 482 630  35 547\n",
      "  231 369 787 619 327 155 666 323 293 209 755 329 512 401 455  27 701 754\n",
      "  620 341  64 767 376 340 385 402 519 306  96  52 780  41 458 447 751 670\n",
      "  660 432  66  84 194 179 377 123  91 263 200 104 384 521 367  75 786 119\n",
      "  500 424 714 336 234 357 187 192]\n",
      " [ 65 511 721 409 430 527 497 614  10 529 131 571 120 784 210 354  73 690\n",
      "  318 557 365 275 128 664 356 775 654 438 480 693 207 459 669 129 189  33\n",
      "  239 681 349 765 602 746 487 575 689 711 561 717 244 328 373 361 730  38\n",
      "  114  14 474 454 335  90 713 426 439 793  48 326 405 483 764 437  68 230\n",
      "  449 653 517 692 358 283 159 481]]\n",
      "reg_coeff: 99999.99999999999, acc: 0.524\n",
      "reg_coeff: 1000.0, acc: 0.653\n",
      "reg_coeff: 100.0, acc: 0.815\n",
      "reg_coeff: 10.0, acc: 0.861\n",
      "reg_coeff: 1.0, acc: 0.870\n",
      "reg_coeff: 0.1, acc: 0.870\n",
      "reg_coeff: 0.05, acc: 0.870\n",
      "reg_coeff: 0.02, acc: 0.870\n",
      "reg_coeff: 0.01, acc: 0.870\n",
      "Best Accuracy: 0.8700 \n",
      "Best Reg: 1\n"
     ]
    }
   ],
   "source": [
    "#We need to learn the model parameter  𝐰 . \n",
    "#However, with different hyperparameters  𝜆 , we can get different model parameter  𝐰 , resulting in different prediction performance. \n",
    "#Thus, we will use the 10-fold cross-validation to select the hyperparameter  𝜆 .\n",
    "\n",
    "#Here we set the folds equal to 10 for 10-fold cross-validation\n",
    "folds = 10\n",
    "\n",
    "#We get the number of samples in the training and validation set\n",
    "num_train_val = X_train_val.shape[0] \n",
    "\n",
    "#Now, we shuffle the index of samples in the train_val set\n",
    "index_of_samples = np.arange(num_train_val) \n",
    "shuffle(index_of_samples)\n",
    "\n",
    "#We split the index of the train_valid set into 10 folds\n",
    "index_of_folds = index_of_samples.reshape(folds, -1)\n",
    "print(index_of_folds)\n",
    "\n",
    "#As suggested above, the hyperparameters chosen are listed below\n",
    "regularization_coefficient = [10**(-5), 10**(-3), 10**(-2), 10**(-1), 1, 10, 20, 50, 100]\n",
    "\n",
    "#Variables we create to store the values of the best accuracy and best regression:\n",
    "best_acc = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "for reg in regularization_coefficient:\n",
    "    #10-fold cross-validation\n",
    "    sum_acc = 0.0\n",
    "    for fold in range(folds):\n",
    "        \n",
    "        index_of_folds_temp = index_of_folds.copy()\n",
    "        \n",
    "        #We are getting the index of the validation set and storing it in a variable valid_index\n",
    "        valid_index = index_of_folds_temp[fold,:].reshape(-1) \n",
    "        #We are getting the index of the training set and storing it in a variable train_index\n",
    "        train_index = np.delete(index_of_folds_temp, fold, 0).reshape(-1)\n",
    "        \n",
    "        #Our training set:\n",
    "        X_train = X_train_val[train_index]\n",
    "        y_train = y_train_val[train_index]\n",
    "        \n",
    "        #Our validation set:\n",
    "        X_valid = X_train_val[valid_index]\n",
    "        y_valid = y_train_val[valid_index]\n",
    "                \n",
    "        #We write this to build the model with different hyperparameters:\n",
    "        clf = LogisticRegression(penalty='l2', C=reg, solver='lbfgs')\n",
    "        \n",
    "        #Train the model with the training set:\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_pred = clf.predict(X_valid)\n",
    "        acc = accuracy_score(y_valid, y_valid_pred)\n",
    "        \n",
    "        sum_acc += acc\n",
    "    \n",
    "    cur_acc = sum_acc / folds\n",
    "    \n",
    "    print(\"reg_coeff: {}, acc: {:.3f}\".format(1.0/reg, cur_acc))\n",
    "    \n",
    "    #We now want to store the best hyperparameter:\n",
    "    if cur_acc > best_acc:\n",
    "        best_acc = cur_acc\n",
    "        best_reg = reg\n",
    "        \n",
    "print(\"Best Accuracy: {:.4f} \".format(best_acc))\n",
    "print(\"Best Reg: {:}\".format(best_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195331a-75a9-4274-8a95-9937b347685d",
   "metadata": {},
   "source": [
    "### Retraining and Evaluating the Model with the Best Hyperparameter: <a id = 'logisticgenderretrain'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab4657f2-bbb0-48ad-b442-2f6807762027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.890, recall: 0.861, precision: 0.916, f1: 0.888,\n",
      "Our learned model parameter vector 'w':  [4.4991212  1.01487776 3.76939602 0.08709338]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now that we have the best hyperparameter, we retrain the model:\n",
    "clf = LogisticRegression(penalty='l2', C=best_reg, solver='lbfgs')\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "#Visualizing the elements of the learned model parameter vector w with a bar plot:\n",
    "print(\"accuracy: {:.3f}, recall: {:.3f}, precision: {:.3f}, f1: {:.3f},\".format(acc, recall, precision, f1))\n",
    "\n",
    "print(\"Our learned model parameter vector 'w': \", abs(clf.coef_[0]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "labels = ['math score', 'reading score', 'writing score', 'average score']\n",
    "coefficients = abs(clf.coef_[0])\n",
    "ax.bar(labels, coefficients)\n",
    "plt.title(\"Bar Graph of Elements of the Learned Model Parameter Vector 'w'\")\n",
    "plt.xlabel('Scores')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a5cdd-5634-4805-9a41-60d2e1b7c03a",
   "metadata": {},
   "source": [
    "From this bar graph, we can see that some scores have much greater coefficients in the learned model parameter than others. Specifically, math and writing scores have the higher coefficient values. This means that these scores were able to fit the logistic regression model with the hyperparameter best of all of the features. One could say that, of the scores provided, the math and writing scores are the best predictors of a student's gender. The closer the coefficient values are to 0, the less correlation exists (that is why we took the absolute value). Thus, the reading and average scores are not good predictors of one's gender. One can then say that there is not a clear distinction between which gender performs better on reading examinations (no correlation). It makes sense for the average score to not be a good predictor of one's gender because this score value is not representative of a single subject/category nor the person's actual score (it is an average). \n",
    "\n",
    "It should be noted that, although we took the absolute value of the coefficients, a positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388c5a0-e364-49fa-b2bc-d147f5ec8a35",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd9b0ef3-51ce-4275-911e-9aabdeb483d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional Imports\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeRegressor # Import Decision Tree Regressor\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03af06f-9932-474f-b7c3-9d2ba8955da5",
   "metadata": {},
   "source": [
    "### Splitting the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55947ec8-4704-41b5-9040-d58e7fc8d602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5)\n",
      "(200, 5)\n"
     ]
    }
   ],
   "source": [
    "#The dataset will be split into a training set with 80% of samples and a test set with 20% of samples\n",
    "#The training set is used to learn model parameters and the testing set is used to evaluate the learned model\n",
    "\n",
    "#Splitting the samples:\n",
    "student_performance_fea = df.drop(['reading score','writing score','math score','average score', 'Math_Pass_Status', 'Reading_Pass_Status', 'Writing_Pass_Status', 'Overall_Pass_Status', 'Grade'], axis=1).values\n",
    "target = df['average score'].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(student_performance_fea,\n",
    "                                                 target,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "#It makes sense that there are 800 values in the training set because this is 80% of the total number of values.\n",
    "#It then also makes sense that there are 200 values in the testing set because this is 20% of the total number of values.\n",
    "\n",
    "#Normalizing the features:\n",
    "normalizer = StandardScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ec525fd-c4e2-459b-bb48-abae3623324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Decision Tree Model through Scikit Learn:\n",
    "\n",
    "# Create Decision Tree regressor object\n",
    "clf = DecisionTreeRegressor()\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "256c42a2-c00b-4d84-890c-1cff94991cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Performance Metrics:\n",
    "def reg_metrics(y_test, y_pred, X_train):\n",
    "    from sklearn.metrics import mean_squared_error, r2_score \n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "    print('Prediction for the testing set:')\n",
    "    print('MAE is: {}'.format(mae))\n",
    "    print('MSE is: {}'.format(mse))\n",
    "    print('RMSE is: {}'.format(rmse))\n",
    "    print('R^2 is: {}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8125029-3c69-42c3-9e55-f67ab507c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the testing set:\n",
      "MAE is: 11.491719664594665\n",
      "MSE is: 212.67447977795615\n",
      "RMSE is: 14.58336311616618\n",
      "R^2 is: -0.005353504025444877\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation:\n",
    "reg_metrics(Y_test, y_pred, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71feec0d-6e9e-40a0-acb8-8c1744aea5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.19.1-py3-none-any.whl (46 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.19.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "223bf364-7ad3-4d34-a1f7-910ecfb6564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydotplus\n",
      "  Downloading pydotplus-2.0.2.tar.gz (278 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\aruna\\anaconda3\\lib\\site-packages (from pydotplus) (2.4.7)\n",
      "Building wheels for collected packages: pydotplus\n",
      "  Building wheel for pydotplus (setup.py): started\n",
      "  Building wheel for pydotplus (setup.py): finished with status 'done'\n",
      "  Created wheel for pydotplus: filename=pydotplus-2.0.2-py3-none-any.whl size=24566 sha256=a805313410b217b2bd77d794a7c5b07b51e4baf488798b919ade19baf107fd3c\n",
      "  Stored in directory: c:\\users\\aruna\\appdata\\local\\pip\\cache\\wheels\\fe\\cd\\78\\a7e873cc049759194f8271f780640cf96b35e5a48bef0e2f36\n",
      "Successfully built pydotplus\n",
      "Installing collected packages: pydotplus\n",
      "Successfully installed pydotplus-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7465cc2d-3040-4893-bc72-a092849ca799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\aruna\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - graphviz\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2022.3.29  |       haa95532_0         122 KB\n",
      "    graphviz-2.38              |       hfd603c8_2        29.3 MB\n",
      "    openssl-1.1.1n             |       h2bbff1b_0         4.8 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        34.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  graphviz           pkgs/main/win-64::graphviz-2.38-hfd603c8_2\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                       2022.2.1-haa95532_0 --> 2022.3.29-haa95532_0\n",
      "  openssl                                 1.1.1m-h2bbff1b_0 --> 1.1.1n-h2bbff1b_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "ca-certificates-2022 | 122 KB    |            |   0% \n",
      "ca-certificates-2022 | 122 KB    | #3         |  13% \n",
      "ca-certificates-2022 | 122 KB    | ########## | 100% \n",
      "\n",
      "openssl-1.1.1n       | 4.8 MB    |            |   0% \n",
      "openssl-1.1.1n       | 4.8 MB    | #          |  10% \n",
      "openssl-1.1.1n       | 4.8 MB    | ##9        |  30% \n",
      "openssl-1.1.1n       | 4.8 MB    | ####6      |  47% \n",
      "openssl-1.1.1n       | 4.8 MB    | ######2    |  63% \n",
      "openssl-1.1.1n       | 4.8 MB    | ########3  |  84% \n",
      "openssl-1.1.1n       | 4.8 MB    | ########## | 100% \n",
      "\n",
      "graphviz-2.38        | 29.3 MB   |            |   0% \n",
      "graphviz-2.38        | 29.3 MB   | 1          |   2% \n",
      "graphviz-2.38        | 29.3 MB   | 5          |   5% \n",
      "graphviz-2.38        | 29.3 MB   | 8          |   9% \n",
      "graphviz-2.38        | 29.3 MB   | #1         |  12% \n",
      "graphviz-2.38        | 29.3 MB   | #4         |  14% \n",
      "graphviz-2.38        | 29.3 MB   | #6         |  17% \n",
      "graphviz-2.38        | 29.3 MB   | #9         |  19% \n",
      "graphviz-2.38        | 29.3 MB   | ##3        |  24% \n",
      "graphviz-2.38        | 29.3 MB   | ##6        |  27% \n",
      "graphviz-2.38        | 29.3 MB   | ##9        |  30% \n",
      "graphviz-2.38        | 29.3 MB   | ###2       |  33% \n",
      "graphviz-2.38        | 29.3 MB   | ###5       |  36% \n",
      "graphviz-2.38        | 29.3 MB   | ###8       |  39% \n",
      "graphviz-2.38        | 29.3 MB   | ####2      |  43% \n",
      "graphviz-2.38        | 29.3 MB   | ####5      |  46% \n",
      "graphviz-2.38        | 29.3 MB   | ####9      |  49% \n",
      "graphviz-2.38        | 29.3 MB   | #####3     |  53% \n",
      "graphviz-2.38        | 29.3 MB   | #####7     |  57% \n",
      "graphviz-2.38        | 29.3 MB   | ######     |  60% \n",
      "graphviz-2.38        | 29.3 MB   | ######4    |  64% \n",
      "graphviz-2.38        | 29.3 MB   | ######8    |  68% \n",
      "graphviz-2.38        | 29.3 MB   | #######2   |  72% \n",
      "graphviz-2.38        | 29.3 MB   | #######5   |  76% \n",
      "graphviz-2.38        | 29.3 MB   | #######8   |  79% \n",
      "graphviz-2.38        | 29.3 MB   | ########2  |  82% \n",
      "graphviz-2.38        | 29.3 MB   | ########5  |  86% \n",
      "graphviz-2.38        | 29.3 MB   | ########8  |  89% \n",
      "graphviz-2.38        | 29.3 MB   | #########2 |  93% \n",
      "graphviz-2.38        | 29.3 MB   | #########6 |  96% \n",
      "graphviz-2.38        | 29.3 MB   | ########## | 100% \n",
      "graphviz-2.38        | 29.3 MB   | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7699c8d7-e6c0-4bfa-a066-c91b10a7e519",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvocationException",
     "evalue": "GraphViz's executables not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvocationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-655a8aa1d6e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 special_characters=True,feature_names = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'])\n\u001b[0;32m     10\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dtr.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(path, f, prog)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                 \u001b[1;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1810\u001b[1;33m                 \u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m             )\n\u001b[0;32m   1812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, path, prog, format)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m                 \u001b[0mfobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1957\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_graphviz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1958\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m                 raise InvocationException(\n\u001b[0m\u001b[0;32m   1960\u001b[0m                     'GraphViz\\'s executables not found')\n\u001b[0;32m   1961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvocationException\u001b[0m: GraphViz's executables not found"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('dtr.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28709f2e-c273-4acc-a0a6-64224949653c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91ea26-b627-4c99-be02-0a12b4fb19fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad69eb01-59cb-4fcb-9ccc-bb020233578f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6843ae4-101f-4edd-a106-6c476e7ba380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0202cbe0-6062-4433-9b00-4d34549d543c",
   "metadata": {},
   "source": [
    "## Random Forest Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38593f83-944c-4a54-82b2-b5409690a644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
